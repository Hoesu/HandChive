{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "955d413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install -q datasets jiwer\n",
    "## !pip install -q transformers\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7068f09",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a210f2fad54ec387a0724bf3d17b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/3.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjdgh\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wjdgh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137f4f9ff0e148e1b0bcfb7412566f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c242559489a4b53b0b92b722a8f5c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjdgh\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"team-lucid/trocr-small-korean\")\n",
    "\n",
    "pixel_values = torch.rand(1, 3, 384, 384)\n",
    "generated_ids = model.generate(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4468e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/wjdgh/Desktop/Recognition_Data/Training/Labels_sample/info.csv\",encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "073025d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EDU_E1_038049_1.jpg</td>\n",
       "      <td>안에서</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EDU_E1_038049_2.jpg</td>\n",
       "      <td>있었지만</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EDU_E1_038049_3.jpg</td>\n",
       "      <td>흔들렸다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDU_E1_038049_4.jpg</td>\n",
       "      <td>흔들</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDU_E1_038049_5.jpg</td>\n",
       "      <td>나는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDU_E1_038049_6.jpg</td>\n",
       "      <td>안전</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EDU_E1_038049_7.jpg</td>\n",
       "      <td>행복버스를</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EDU_E1_038049_8.jpg</td>\n",
       "      <td>탔다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EDU_E1_038049_9.jpg</td>\n",
       "      <td>버스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EDU_E1_038049_10.jpg</td>\n",
       "      <td>교통</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EDU_E1_038049_11.jpg</td>\n",
       "      <td>지진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EDU_E1_038049_12.jpg</td>\n",
       "      <td>화재</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EDU_E1_038049_13.jpg</td>\n",
       "      <td>체험</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EDU_E1_038049_14.jpg</td>\n",
       "      <td>을</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EDU_E1_038049_15.jpg</td>\n",
       "      <td>했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EDU_E1_038049_16.jpg</td>\n",
       "      <td>차가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EDU_E1_038049_17.jpg</td>\n",
       "      <td>멈춰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>EDU_E1_038049_20.jpg</td>\n",
       "      <td>실제로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EDU_E1_038049_21.jpg</td>\n",
       "      <td>움직이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>EDU_E1_038049_22.jpg</td>\n",
       "      <td>는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EDU_E1_038049_23.jpg</td>\n",
       "      <td>것처럼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>EDU_E1_038049_24.jpg</td>\n",
       "      <td>의자가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>EDU_E1_038049_25.jpg</td>\n",
       "      <td>안전벨트를</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>EDU_E1_038049_26.jpg</td>\n",
       "      <td>했더니</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>EDU_E1_038049_27.jpg</td>\n",
       "      <td>다치지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>EDU_E1_038049_28.jpg</td>\n",
       "      <td>않</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>EDU_E1_038049_29.jpg</td>\n",
       "      <td>았다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>EDU_E1_038049_30.jpg</td>\n",
       "      <td>지진이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>EDU_E1_038049_31.jpg</td>\n",
       "      <td>난</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EDU_E1_038049_32.jpg</td>\n",
       "      <td>것처럼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>EDU_E1_038049_33.jpg</td>\n",
       "      <td>버스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>EDU_E1_038049_34.jpg</td>\n",
       "      <td>바닥이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>EDU_E1_038205_1.jpg</td>\n",
       "      <td>근데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>EDU_E1_038205_2.jpg</td>\n",
       "      <td>돈을</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>EDU_E1_038205_3.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>EDU_E1_038205_4.jpg</td>\n",
       "      <td>만원</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>EDU_E1_038205_5.jpg</td>\n",
       "      <td>밖</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>EDU_E1_038205_6.jpg</td>\n",
       "      <td>안</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>EDU_E1_038205_7.jpg</td>\n",
       "      <td>가지고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>EDU_E1_038205_8.jpg</td>\n",
       "      <td>왔다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>EDU_E1_038205_9.jpg</td>\n",
       "      <td>그@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>EDU_E1_038205_10.jpg</td>\n",
       "      <td>엄마가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>EDU_E1_038205_11.jpg</td>\n",
       "      <td>꼭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>EDU_E1_038205_12.jpg</td>\n",
       "      <td>막사지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>EDU_E1_038205_14.jpg</td>\n",
       "      <td>진짜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>EDU_E1_038205_15.jpg</td>\n",
       "      <td>필요한것만</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>EDU_E1_038205_16.jpg</td>\n",
       "      <td>사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>EDU_E1_038205_17.jpg</td>\n",
       "      <td>라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>EDU_E1_038205_18.jpg</td>\n",
       "      <td>하셨다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>EDU_E1_038205_19.jpg</td>\n",
       "      <td>그@서</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name   text\n",
       "0    EDU_E1_038049_1.jpg    안에서\n",
       "1    EDU_E1_038049_2.jpg   있었지만\n",
       "2    EDU_E1_038049_3.jpg   흔들렸다\n",
       "3    EDU_E1_038049_4.jpg     흔들\n",
       "4    EDU_E1_038049_5.jpg     나는\n",
       "5    EDU_E1_038049_6.jpg     안전\n",
       "6    EDU_E1_038049_7.jpg  행복버스를\n",
       "7    EDU_E1_038049_8.jpg     탔다\n",
       "8    EDU_E1_038049_9.jpg     버스\n",
       "9   EDU_E1_038049_10.jpg     교통\n",
       "10  EDU_E1_038049_11.jpg     지진\n",
       "11  EDU_E1_038049_12.jpg     화재\n",
       "12  EDU_E1_038049_13.jpg     체험\n",
       "13  EDU_E1_038049_14.jpg      을\n",
       "14  EDU_E1_038049_15.jpg     했다\n",
       "15  EDU_E1_038049_16.jpg     차가\n",
       "16  EDU_E1_038049_17.jpg     멈춰\n",
       "17  EDU_E1_038049_20.jpg    실제로\n",
       "18  EDU_E1_038049_21.jpg    움직이\n",
       "19  EDU_E1_038049_22.jpg      는\n",
       "20  EDU_E1_038049_23.jpg    것처럼\n",
       "21  EDU_E1_038049_24.jpg    의자가\n",
       "22  EDU_E1_038049_25.jpg  안전벨트를\n",
       "23  EDU_E1_038049_26.jpg    했더니\n",
       "24  EDU_E1_038049_27.jpg    다치지\n",
       "25  EDU_E1_038049_28.jpg      않\n",
       "26  EDU_E1_038049_29.jpg     았다\n",
       "27  EDU_E1_038049_30.jpg    지진이\n",
       "28  EDU_E1_038049_31.jpg      난\n",
       "29  EDU_E1_038049_32.jpg    것처럼\n",
       "30  EDU_E1_038049_33.jpg     버스\n",
       "31  EDU_E1_038049_34.jpg    바닥이\n",
       "32   EDU_E1_038205_1.jpg     근데\n",
       "33   EDU_E1_038205_2.jpg     돈을\n",
       "34   EDU_E1_038205_3.jpg      4\n",
       "35   EDU_E1_038205_4.jpg     만원\n",
       "36   EDU_E1_038205_5.jpg      밖\n",
       "37   EDU_E1_038205_6.jpg      안\n",
       "38   EDU_E1_038205_7.jpg    가지고\n",
       "39   EDU_E1_038205_8.jpg    왔다.\n",
       "40   EDU_E1_038205_9.jpg     그@\n",
       "41  EDU_E1_038205_10.jpg    엄마가\n",
       "42  EDU_E1_038205_11.jpg      꼭\n",
       "43  EDU_E1_038205_12.jpg    막사지\n",
       "44  EDU_E1_038205_14.jpg     진짜\n",
       "45  EDU_E1_038205_15.jpg  필요한것만\n",
       "46  EDU_E1_038205_16.jpg      사\n",
       "47  EDU_E1_038205_17.jpg      라\n",
       "48  EDU_E1_038205_18.jpg   하셨다.\n",
       "49  EDU_E1_038205_19.jpg    그@서"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd59dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc707c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class HandWriting(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeae511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = HandWriting(root_dir='C:/Users/wjdgh/Desktop/Recognition_Data/Training/Images_sample/',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = HandWriting(root_dir='C:/Users/wjdgh/Desktop/Recognition_Data/Training/Images_sample/',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cce528c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 221\n",
      "Number of validation examples: 56\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69a7ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[20]\n",
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "179649ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=183x155 at 0x19387A07640>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_dataset.root_dir + train_df['file_name'][20]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f642c11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교통\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78517d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "514c6c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): DeiTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): Embedding(50265, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b86c1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "028c8a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjdgh\\AppData\\Local\\Temp\\ipykernel_39920\\152175726.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83c3bdcf76947b59474a10d70f75d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a1991e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7543ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjdgh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9de0fae29a4e0d83ae878cce3906d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m     11\u001b[0m    \u001b[38;5;66;03m# get the inputs\u001b[39;00m\n\u001b[0;32m     12\u001b[0m    \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     13\u001b[0m      batch[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mHandWriting.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# prepare image (i.e. resize + normalize)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir \u001b[38;5;241m+\u001b[39m file_name)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpixel_values\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# add labels (input_ids) by encoding the text\u001b[39;00m\n\u001b[0;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer(text, \n\u001b[0;32m     24\u001b[0m                                   padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     25\u001b[0m                                   max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_target_length)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:83\u001b[0m, in \u001b[0;36mTrOCRProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `images` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(images, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\image_processing_utils.py:494\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m    493\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:259\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRescale factor must be specified if do_rescale is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m    262\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize_dict, resample\u001b[38;5;241m=\u001b[39mresample) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:259\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRescale factor must be specified if do_rescale is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[43mto_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m    262\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize_dict, resample\u001b[38;5;241m=\u001b[39mresample) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\image_utils.py:142\u001b[0m, in \u001b[0;36mto_numpy_array\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_valid_image(img):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_vision_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_numpy(img)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:547\u001b[0m, in \u001b[0;36mis_vision_available\u001b[1;34m()\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pil_available:\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m         package_version \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPillow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\importlib\\metadata.py:569\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m    563\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\importlib\\metadata.py:542\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\importlib\\metadata.py:192\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resolver \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_discover_resolvers():\n\u001b[0;32m    191\u001b[0m     dists \u001b[38;5;241m=\u001b[39m resolver(DistributionFinder\u001b[38;5;241m.\u001b[39mContext(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m--> 192\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\setuptools\\_vendor\\importlib_metadata\\__init__.py:894\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find metadata directories in paths heuristically.\"\"\"\u001b[39;00m\n\u001b[0;32m    892\u001b[0m prepared \u001b[38;5;241m=\u001b[39m Prepared(name)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m--> 894\u001b[0m     \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(FastPath, paths)\n\u001b[0;32m    895\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\setuptools\\_vendor\\importlib_metadata\\__init__.py:786\u001b[0m, in \u001b[0;36mFastPath.search\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtime\u001b[49m)\u001b[38;5;241m.\u001b[39msearch(name)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\setuptools\\_vendor\\importlib_metadata\\__init__.py:791\u001b[0m, in \u001b[0;36mFastPath.mtime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmtime\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[1;32m--> 791\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_mtime\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup\u001b[38;5;241m.\u001b[39mcache_clear()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "   # train\n",
    "   model.train()\n",
    "   train_loss = 0.0\n",
    "   for batch in tqdm(train_dataloader):\n",
    "      # get the inputs\n",
    "      for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "   # evaluate\n",
    "   model.eval()\n",
    "   valid_cer = 0.0\n",
    "   with torch.no_grad():\n",
    "     for batch in tqdm(eval_dataloader):\n",
    "       # run batch generation\n",
    "       outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "       # compute metrics\n",
    "       cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "       valid_cer += cer \n",
    "\n",
    "   print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
    "\n",
    "model.save_pretrained(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e617d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
