{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5632c0a",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "Transformers (for the TrOCR model) </br>\n",
    "Datasets & Jiwer (for the evaluation metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vessl Requirements\n",
    "##!pip install torch\n",
    "##!pip install transformers==4.28.0\n",
    "##!pip install datasets jiwer\n",
    "##!pip install ipywidgets==7.5.1\n",
    "##!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "## !pip install torchvision\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec33b3c",
   "metadata": {},
   "source": [
    "## 데이터 프로세서와 모델 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787869a",
   "metadata": {},
   "source": [
    "모델에 먹여주기 위한 데이터를 준비하기 위해 TrOCRProcessor를 사용합니다. </br>\n",
    "TrOCRProcessor는 ViTFeatureExtractor와 RobertaTokenizer를 포함하는 래퍼입니다. </br>\n",
    "ViTFeatureExtractor: 이미지 Resize & Normalization. </br>\n",
    "RobertaTokenizer   : 텍스트 encoding & decoding. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e9694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor, AutoTokenizer, DeiTImageProcessor\n",
    "\n",
    "## 이미지 프로세서와 토크나이저 사전 훈련 모델이 사용한 것과 똑같은 것 가져오기.\n",
    "DEIT = DeiTImageProcessor.from_pretrained(\"team-lucid/trocr-small-korean\")\n",
    "ROBERTA = AutoTokenizer.from_pretrained(\"team-lucid/trocr-small-korean\")\n",
    "processor = TrOCRProcessor(image_processor = DEIT, tokenizer = ROBERTA)\n",
    "## 사전 훈련 모델 가져오기.\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"team-lucid/trocr-small-korean\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0a459",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917548e",
   "metadata": {},
   "source": [
    "테스트용 샘플 데이터를 불러옵니다. (추후에 모델 연결 과정을 위해 수정할 예정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('압축 폴더 위치', 'r') as zip_ref:\n",
    "    zip_ref.extractall('압축 풀고 저장할 경로')\n",
    "\n",
    "## 이미지 파일 이름과 텍스트 레이블이 저장된 CSV 파일 불러오기\n",
    "df = pd.read_csv('레이블 CSV 위치', encoding='UTF-8')\n",
    "## 이미지 파일 불러올 디렉토리 루트로 설정\n",
    "root_dir='이미지 불러올 디렉토리 위치/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fca7ab",
   "metadata": {},
   "source": [
    "데이터셋은 두가지를 리턴합니다. </br>\n",
    "pixel_values: 이미지 피쳐 </br>\n",
    "labels      : 텍스트 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57c7aa",
   "metadata": {},
   "source": [
    "##  데이터셋 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class HandWriting(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=50):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        ## 한줄에 많은 수의 단어가 들어오진 않으니\n",
    "        ## 적당하게 50토큰 정도로 지정해놓자. (대략 30~40 단어)\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ## 파일 이름과 텍스트 레이블 가져오기.\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        ## 프로세서를 통해 이미지 인코딩.\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        ## 토크나이저를 통해 텍스트 레이블을 인코딩.\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        ## 패딩 토큰은 비용 함수가 무시하게끔 설정하기.\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        ## 변환한 이미지와 텍스트에 대한 인코딩을 딕셔너리에 저장. 모델 입력값으로 쓰일 예정.\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e469ba",
   "metadata": {},
   "source": [
    "## 데이터 스플릿 & 데이터 로더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## 데이터 스플릿\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state = 0)\n",
    "## 인덱스 초기화\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "## 데이터셋 클래스에 먹여주기\n",
    "train_dataset = HandWriting(root_dir=root_dir, df=train_df, processor=processor)\n",
    "test_dataset = HandWriting(root_dir=root_dir, df=test_df, processor=processor)\n",
    "## 데이터 로더\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd68d592",
   "metadata": {},
   "source": [
    "## 중간 점검"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df57b7e",
   "metadata": {},
   "source": [
    "데이터셋 스플릿 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302fb66",
   "metadata": {},
   "source": [
    "이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623488ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994850a",
   "metadata": {},
   "source": [
    "레이블 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d623b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fed25",
   "metadata": {},
   "source": [
    "프로세서로 인코딩 제대로 하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc006",
   "metadata": {},
   "source": [
    "레이블 디코딩 제대로 되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af7f1e",
   "metadata": {},
   "source": [
    "## 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ff76d",
   "metadata": {},
   "source": [
    "## 평가 지표 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f194da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff85ffa",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "   # train\n",
    "   model.train()\n",
    "   train_loss = 0.0\n",
    "   for batch in tqdm(train_dataloader):\n",
    "      # get the inputs\n",
    "      for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "   # evaluate\n",
    "   model.eval()\n",
    "   valid_cer = 0.0\n",
    "   with torch.no_grad():\n",
    "     for batch in tqdm(test_dataloader):\n",
    "       # run batch generation\n",
    "       outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "       # compute metrics\n",
    "       cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "       valid_cer += cer \n",
    "\n",
    "   print(\"Validation CER:\", valid_cer / len(test_dataloader))\n",
    "\n",
    "torch.save(model, '저장할 위치')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f72a5",
   "metadata": {},
   "source": [
    "## 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eee6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Model/model.pth'\n",
    "model = torch.load(model_path)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbb65b",
   "metadata": {},
   "source": [
    "## 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03842c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir='인퍼런스 진행할 이미지 들어있는 폴더 위치/'\n",
    "lines = os.listdir(root_dir)\n",
    "for line in lines:\n",
    "\timage = Image.open(root_dir + line).convert(\"RGB\")\n",
    "\tpixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\tpixel_values = pixel_values.to('cuda')\n",
    "\tgenerated_ids = model.generate(pixel_values)\n",
    "\tgenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\tprint(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
